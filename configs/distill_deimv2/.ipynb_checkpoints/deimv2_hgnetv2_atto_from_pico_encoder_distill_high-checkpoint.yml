__include__: [
  '../deimv2/deimv2_hgnetv2_atto_coco.yml'
]

output_dir: ./outputs/distill_deimv2/hgnetv2_atto_from_pico_encoder_high

model: DEIMFGDDistiller

last_epoch: 468
epochs: 510

DEIMFGDDistiller:
  student_ckpt: ./best/deimv2_hgnetv2_atto_coco.pth
  student:
    type: DEIM
    backbone:
      type: HGNetv2
      name: 'Atto'
      return_idx: [2]
      freeze_at: -1
      freeze_norm: false
      use_lab: true
    encoder:
      type: LiteEncoder
      in_channels: [256]
      feat_strides: [16]
      hidden_dim: 64
      expansion: 0.34
      depth_mult: 0.5
      act: silu
    decoder:
      type: DEIMTransformer
      feat_channels: [64, 64]
      feat_strides: [16, 32]
      hidden_dim: 64
      num_levels: 2
      num_points: [4, 2]
      dim_feedforward: 160
      num_layers: 3
      eval_idx: -1
      num_queries: 100
  teacher:
    type: DEIM
    backbone:
      type: HGNetv2
      name: 'Pico'
      return_idx: [2]
      freeze_at: -1
      freeze_norm: false
      use_lab: true
    encoder:
      type: LiteEncoder
      in_channels: [512]
      feat_strides: [16]
      hidden_dim: 112
      expansion: 0.34
      depth_mult: 0.5
      act: silu
    decoder:
      type: DEIMTransformer
      feat_channels: [112, 112]
      feat_strides: [16, 32]
      hidden_dim: 112
      num_levels: 2
      num_points: [4, 2]
      dim_feedforward: 320
      num_layers: 3
      eval_idx: -1
      num_queries: 200
      share_bbox_head: true
      use_gateway: false
  feature_pairs:
    - name: c4
      student_index: 0
      teacher_index: 0
      meta:
        source: encoder
        start_epoch: 468
  teacher_ckpt: ./best/deimv2_hgnetv2_pico_coco.pth

DEIMCriterion:
  distill_cfg:
    - name: c4
      weight: 1
      start_epoch: 468
      ramp_epochs: 16
      start_weight: 0.26
      loss:
        type: FGDFeatureLoss
        student_channels: 64
        teacher_channels: 112
        temp: 0.9
        alpha_fgd: 0.00028
        beta_fgd: 0.00005
        gamma_fgd: 0.00016
        lambda_fgd: 0.000001

train_dataloader:
  total_batch_size: 96 # total batch size equals to 32 (4 * 8)
  num_workers: 8
  collate_fn:
    stop_epoch: 468
